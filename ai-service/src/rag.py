from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_tavily import TavilySearch
from langchain_core.tools import tool
from langchain_classic.agents import create_openai_tools_agent, AgentExecutor
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.globals import set_llm_cache
from langchain_community.cache import InMemoryCache
from src.config.env import settings
from typing import Any, Iterable, Optional, Sequence
import os

# --- 1. C·∫§U H√åNH C∆† B·∫¢N ---
# Setup Tavily Key
os.environ["TAVILY_API_KEY"] = settings.TAVILY_API_KEY

set_llm_cache(InMemoryCache())

# Setup Embeddings
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cpu'}
)

# Setup LLM (Gemini)
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash", # Ho·∫∑c 1.5-flash
    google_api_key=settings.GOOGLE_API_KEY,
    temperature=0 # ƒê·ªÉ Agent ra quy·∫øt ƒë·ªãnh ch√≠nh x√°c, n√™n ƒë·ªÉ temp th·∫•p
)

# --- 2. B·ªò NH·ªö (MEMORY) ---
# V·∫´n d√πng c√°ch l∆∞u dictionary ƒë∆°n gi·∫£n c·ªßa b·∫°n, nh∆∞ng t√≠ n·ªØa s·∫Ω convert
CHAT_HISTORY = {}

def get_chat_history(session_id):
    return CHAT_HISTORY.get(session_id, [])

def save_chat_history(session_id, question, answer):
    if session_id not in CHAT_HISTORY: CHAT_HISTORY[session_id] = []
    CHAT_HISTORY[session_id].append((question, answer))
    if len(CHAT_HISTORY[session_id]) > 10: CHAT_HISTORY[session_id].pop(0)

def build_langchain_history(history_payload: Optional[Iterable[Any]], session_id: str):
    lc_history = []

    if history_payload:
        for entry in history_payload:
            sender = None
            content = None

            if isinstance(entry, dict):
                sender = entry.get("sender")
                content = entry.get("content")
            else:
                sender = getattr(entry, "sender", None)
                content = getattr(entry, "content", None)

            if not content:
                continue

            if str(sender).upper() == "USER":
                lc_history.append(HumanMessage(content=content))
            else:
                lc_history.append(AIMessage(content=content))

    else:
        raw_history = get_chat_history(session_id)
        for q, a in raw_history:
            lc_history.append(HumanMessage(content=q))
            lc_history.append(AIMessage(content=a))

    return lc_history[-6:]

# --- 3. ƒê·ªäNH NGHƒ®A C√îNG C·ª§ (TOOLS) ---
# Agent s·∫Ω nh√¨n v√†o docstring ("""...""") ƒë·ªÉ bi·∫øt khi n√†o d√πng tool n√†o.

@tool
def lookup_grammar_book(query: str):
    """
    D√πng c√¥ng c·ª• n√†y ƒë·ªÉ tra c·ª©u ki·∫øn th·ª©c v·ªÅ Ng·ªØ ph√°p Ti·∫øng Anh (Grammar), 
    c·∫•u tr√∫c c√¢u (Sentence Structure), c√°c th√¨ (Tenses) trong s√°ch gi√°o khoa.
    """
    print(f"üìò [Tool] ƒêang tra s√°ch Ng·ªØ ph√°p: {query}")
    try:
        vector_store = Chroma(
            persist_directory=settings.CHROMA_DB_DIR,
            embedding_function=embedding_model,
            collection_name="grammar_collection"
        )
        retriever = vector_store.as_retriever(search_kwargs={"k": 4})
        docs = retriever.invoke(query)
        return "\n\n".join([doc.page_content for doc in docs])
    except Exception as e:
        print(f"‚ùå L·ªói khi tra s√°ch Ng·ªØ ph√°p: {e}")
        return "S√°ch gi√°o khoa kh√¥ng ƒë·ªÅ c·∫≠p chi ti·∫øt. H√£y s·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n ƒë·ªÉ gi·∫£i th√≠ch ƒë·∫ßy ƒë·ªß cho h·ªçc vi√™n."

@tool
def lookup_vocab_book(query: str):
    """
    D√πng c√¥ng c·ª• n√†y ƒë·ªÉ tra c·ª©u T·ª´ v·ª±ng (Vocabulary), ƒë·ªãnh nghƒ©a t·ª´ (Definition),
    th√†nh ng·ªØ (Idioms) ho·∫∑c c·ª•m t·ª´ trong s√°ch gi√°o khoa.
    """
    print(f"üìó [Tool] ƒêang tra s√°ch T·ª´ v·ª±ng: {query}")
    try:
        vector_store = Chroma(
            persist_directory=settings.CHROMA_DB_DIR,
            embedding_function=embedding_model,
            collection_name="vocab_collection"
        )
        retriever = vector_store.as_retriever(search_kwargs={"k": 4})
        docs = retriever.invoke(query)
        return "\n\n".join([doc.page_content for doc in docs])
    except Exception as e:
        print(f"‚ùå L·ªói khi tra s√°ch T·ª´ v·ª±ng: {e}")
        return "S√°ch gi√°o khoa kh√¥ng ƒë·ªÅ c·∫≠p chi ti·∫øt. H√£y s·ª≠ d·ª•ng ki·∫øn th·ª©c chuy√™n m√¥n c·ªßa b·∫°n ƒë·ªÉ gi·∫£i th√≠ch ƒë·∫ßy ƒë·ªß cho h·ªçc vi√™n."

# Tool Search Google (Tavily)
search_web_tool = TavilySearch(
    max_results=3,
    description="D√πng c√¥ng c·ª• n√†y ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin KH√îNG c√≥ trong s√°ch gi√°o khoa, ki·∫øn th·ª©c x√£ h·ªôi, ho·∫∑c c√°c t·ª´ l√≥ng (slang) m·ªõi nh·∫•t."
)

# Gom t·∫•t c·∫£ tools l·∫°i
tools = [lookup_grammar_book, lookup_vocab_book, search_web_tool]

# --- 4. T·∫†O AGENT ---
def create_lingora_agent():
    # Prompt System cho Agent
    system_prompt = """
    B·∫°n l√† LingoraBot - Tr·ª£ l√Ω ·∫£o d·∫°y Ti·∫øng Anh.
    B·∫°n c√≥ 3 c√¥ng c·ª•: S√°ch Ng·ªØ Ph√°p, S√°ch T·ª´ V·ª±ng, Google Search.

    NHI·ªÜM V·ª§ DUY NH·∫§T:
    - X·ª≠ l√Ω v√† tr·∫£ l·ªùi c√¢u h·ªèi M·ªöI NH·∫§T c·ªßa ng∆∞·ªùi d√πng (n·∫±m trong bi·∫øn input).

    üî¥ QUY T·∫ÆC "V√ÄNG" KHI D√ôNG L·ªäCH S·ª¨ (IGNORE HISTORY CONTENT):
    1. **IGNORE PREVIOUS ANSWERS:** L·ªãch s·ª≠ chat ch·ªâ ƒë·ªÉ b·∫°n hi·ªÉu ng·ªØ c·∫£nh (v√≠ d·ª• user n√≥i "n√≥ l√† g√¨" th√¨ t√¨m trong l·ªãch s·ª≠ xem "n√≥" l√† g√¨).
    2. **C·∫§M L·∫∂P L·∫†I:** Tuy·ªát ƒë·ªëi KH√îNG nh·∫Øc l·∫°i, kh√¥ng t√≥m t·∫Øt, kh√¥ng copy-paste b·∫•t k·ª≥ n·ªôi dung n√†o c·ªßa c√°c c√¢u tr·∫£ l·ªùi tr∆∞·ªõc ƒë√≥.
    3. **C√ÇU TR·∫¢ L·ªúI ƒê·ªòC L·∫¨P:** C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n ph·∫£i m·ªõi ho√†n to√†n, ƒëi th·∫≥ng v√†o v·∫•n ƒë·ªÅ c·ªßa c√¢u h·ªèi m·ªõi. Kh√¥ng b·∫Øt ƒë·∫ßu b·∫±ng "Nh∆∞ ƒë√£ n√≥i...", "V·ªÅ c√¢u h·ªèi tr∆∞·ªõc...".

    QUY T·∫ÆC KH√ÅC:
    - Tr·∫£ l·ªùi b·∫±ng Ti·∫øng Vi·ªát t·ª± nhi√™n, th√¢n thi·ªán.
    - N·∫øu s√°ch kh√¥ng c√≥, d√πng ki·∫øn th·ª©c c·ªßa b·∫°n, KH√îNG ƒë∆∞·ª£c xin l·ªói.
    - Kh√¥ng nh·∫Øc t√™n c√¥ng c·ª• (lookup...).
    """
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"), # N∆°i nh√©t l·ªãch s·ª≠ v√†o
        ("human", "{input}"),
        MessagesPlaceholder(variable_name="agent_scratchpad"), # N∆°i Agent suy nghƒ©
    ])

    # T·∫°o Agent
    agent = create_openai_tools_agent(llm, tools, prompt)
    
    # Executor l√† b·ªô m√°y ch·∫°y Agent
    agent_executor = AgentExecutor(
        agent=agent, 
        tools=tools, 
        verbose=True, # B·∫≠t True ƒë·ªÉ nh√¨n th·∫•y suy nghƒ© c·ªßa Agent tr√™n terminal
        handle_parsing_errors=True
    )
    return agent_executor

# Kh·ªüi t·∫°o 1 l·∫ßn d√πng chung
lingora_agent = create_lingora_agent()

# --- 5. H√ÄM CH√çNH (ƒê∆Ø·ª¢C G·ªåI T·ª™ API) ---
def get_answer(question: str, type: str = None, session_id: str = "default", history: Optional[Sequence[dict]] = None):
    lc_history = build_langchain_history(history, session_id)
    
    print(f"ü§ñ Agent ƒëang suy nghƒ© cho session: {session_id}...; c√≥ history: {len(lc_history)}")

    try:
        # 3. Ch·∫°y Agent
        print(f"question: {question}")
        result = lingora_agent.invoke({
            "input": question,
            "chat_history": lc_history
        })
        print(f"result: {result}")
        raw_output = result['output']
        final_response = ""
        # Tr∆∞·ªùng h·ª£p 1: N√≥ tr·∫£ v·ªÅ chu·ªói b√¨nh th∆∞·ªùng (Ngon)
        if isinstance(raw_output, str):
            final_response = raw_output
            
        # Tr∆∞·ªùng h·ª£p 2: N√≥ tr·∫£ v·ªÅ List (nh∆∞ c√°i l·ªói b·∫°n g·∫∑p)
        elif isinstance(raw_output, list):
            for part in raw_output:
                # N·∫øu l√† chu·ªói th√¨ c·ªông v√†o
                if isinstance(part, str):
                    final_response += part
                # N·∫øu l√† Dictionary (c√≥ ch·ª©a 'text') th√¨ l·∫•y ph·∫ßn text
                elif isinstance(part, dict) and 'text' in part:
                    final_response += part['text']
        
        # Tr∆∞·ªùng h·ª£p 3: N√≥ tr·∫£ v·ªÅ Object l·∫° -> √âp sang string
        else:
            final_response = str(raw_output)
        # 4. L∆∞u l·∫°i l·ªãch s·ª≠
        if history is None:
            save_chat_history(session_id, question, final_response)
        
        return final_response

    except Exception as e:
        print(f"‚ùå Agent Error: {e}")
        return "Xin l·ªói, h·ªá th·ªëng ƒëang g·∫∑p ch√∫t tr·ª•c tr·∫∑c khi suy nghƒ©. B·∫°n h·ªèi l·∫°i th·ª≠ xem?"

def generate_chat_title(question: str):
    prompt = f"""
    Nhi·ªám v·ª•: T√≥m t·∫Øt c√¢u h·ªèi sau th√†nh m·ªôt TI√äU ƒê·ªÄ ng·∫Øn g·ªçn, s√∫c t√≠ch (d∆∞·ªõi 6 t·ª´).
    Y√™u c·∫ßu:
    - B·ªè c√°c t·ª´ th·ª´a nh∆∞ "cho m√¨nh h·ªèi", "l√†m sao ƒë·ªÉ", "l√† g√¨".
    - Gi·ªØ l·∫°i t·ª´ kh√≥a ch√≠nh.
    - Vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu.
    - V√≠ d·ª•: "Th√¨ hi·ªán t·∫°i ƒë∆°n d√πng khi n√†o" -> "C√°ch d√πng th√¨ Hi·ªán t·∫°i ƒë∆°n"
    
    C√¢u h·ªèi: "{question}"
    
    Ti√™u ƒë·ªÅ:
    """
    try:
        # G·ªçi LLM (d√πng bi·∫øn llm ƒë√£ khai b√°o ·ªü tr√™n)
        title = llm.invoke(prompt).content
        
        # L√†m s·∫°ch chu·ªói (b·ªè ngo·∫∑c k√©p, kho·∫£ng tr·∫Øng th·ª´a)
        return title.strip().replace('"', '').replace("'", "")
    except Exception:
        # Fallback n·∫øu AI l·ªói: C·∫Øt chu·ªói th·ªß c√¥ng
        return question[:50] + "..."