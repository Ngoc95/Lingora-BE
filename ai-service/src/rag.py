from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_classic.chains import RetrievalQA
from config.env import settings
import os
from typing import Optional
# Cáº¥u hÃ¬nh Embeddings (Pháº£i GIá»NG Há»†T lÃºc náº¡p dá»¯ liá»‡u)
embedding_model = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
    model_kwargs={'device': 'cpu'}
)

# Cáº¥u hÃ¬nh LLM (Gemini)
# Temperature = 0.3 Ä‘á»ƒ cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, Ã­t bá»‹a Ä‘áº·t
llm = GoogleGenerativeAI(
    model="gemini-2.0-flash",
    google_api_key=settings.GOOGLE_API_KEY,
    temperature=0.4
)
# --- 2. Bá»˜ NHá»š (MEMORY) ---
CHAT_HISTORY = {}

def get_chat_history(session_id):
    return CHAT_HISTORY.get(session_id, [])

def save_chat_history(session_id, question, answer):
    if session_id not in CHAT_HISTORY: CHAT_HISTORY[session_id] = []
    CHAT_HISTORY[session_id].append((question, answer))
    if len(CHAT_HISTORY[session_id]) > 5: CHAT_HISTORY[session_id].pop(0)

# --- 3. Xá»¬ LÃ XÃƒ GIAO (CHITCHAT) - QUAN TRá»ŒNG ---
def is_chitchat(question):
    """
    DÃ¹ng AI Ä‘á»ƒ phÃ¢n loáº¡i xem Ä‘Ã¢y lÃ  cÃ¢u chÃ o há»i xÃ£ giao (Chitchat) 
    hay lÃ  cÃ¢u há»i cáº§n tra cá»©u kiáº¿n thá»©c (Learning).
    """
    prompt = f"""
    Classify the user input into one of two categories: 'chitchat' or 'learning'.
    
    Definitions:
    - 'chitchat': Greetings (Hello, Hi), personal questions about the bot (Who are you?), closing (Bye), gratitude (Thanks), or general small talk.
    - 'learning': Questions asking for knowledge about English, Grammar, Vocabulary, definitions, examples, translations, or how to use words.
    
    CRITICAL RULE: 
    If the input contains BOTH a greeting and a learning question (e.g., "Hello, what is a noun?"), classify it as 'learning'.
    
    Input: "{question}"
    
    Return ONLY one word: 'chitchat' or 'learning'.
    """
    try:
        # Gá»i Gemini Ä‘á»ƒ phÃ¢n loáº¡i (nhanh gá»n)
        result = llm.invoke(prompt).strip().lower()
        
        # In ra log Ä‘á»ƒ báº¡n theo dÃµi nÃ³ quyáº¿t Ä‘á»‹nh tháº¿ nÃ o
        print(f"ğŸ¤– Intent Classifier: '{question}' -> {result.upper()}")
        
        if "chitchat" in result:
            return True
        return False # Máº·c Ä‘á»‹nh lÃ  'learning' Ä‘á»ƒ Ä‘i tra sÃ¡ch
    except Exception as e:
        print(f"âš ï¸ Lá»—i phÃ¢n loáº¡i, máº·c Ä‘á»‹nh tra sÃ¡ch: {e}")
        return False

def handle_chitchat(question, history):
    """Tráº£ lá»i xÃ£ giao thÃ¢n thiá»‡n"""
    history_text = "\n".join([f"User: {q}\nBot: {a}" for q, a in history])
    
    prompt = f"""
    Báº¡n lÃ  trá»£ lÃ½ áº£o há»c táº­p tÃªn lÃ  "Lingora". TÃ­nh cÃ¡ch: ThÃ¢n thiá»‡n, hÃ i hÆ°á»›c, lá»… phÃ©p.
    
    [Lá»‹ch sá»­ chat]:
    {history_text}
    
    [User nÃ³i]: "{question}"
    
    HÃ£y tráº£ lá»i ngÆ°á»i dÃ¹ng má»™t cÃ¡ch tá»± nhiÃªn báº±ng tiáº¿ng Viá»‡t (khÃ´ng cáº§n tra kiáº¿n thá»©c).
    Náº¿u há» chÃ o, hÃ£y chÃ o láº¡i vÃ  má»i há» Ä‘áº·t cÃ¢u há»i vá» Tiáº¿ng Anh.
    """
    return llm.invoke(prompt)

# --- 4. HÃ€M VIáº¾T Láº I CÃ‚U Há»I & DETECT INTENT (Giá»¯ nguyÃªn logic cÅ©) ---
def contextualize_query(question, history):
    if not history: return question
    
    # Chá»‰ láº¥y 2 lÆ°á»£t há»i Ä‘Ã¡p gáº§n nháº¥t Ä‘á»ƒ trÃ¡nh nhiá»…u thÃ´ng tin quÃ¡ cÅ©
    recent_history = history[-2:] 
    history_str = "\n".join([f"User: {q}\nAI: {a}" for q, a in recent_history])
    
    prompt = f"""
    [Chat History]:
    {history_str}
    
    [User's Input]:
    {question}
    
    TASK: Rewrite the User's Input to be a standalone question that can be understood without the chat history.
    RULE: Keep the original intent EXACTLY. Do not narrow down the scope unless the user explicitly asks to.
    
    [Rewritten Question]:
    """
    try:
        return llm.invoke(prompt).strip()
    except:
        return question

def detect_intent(question):
    # Logic cÅ©
    keywords = ["nghÄ©a", "mean", "vocab", "tá»« vá»±ng", "Ä‘á»‹nh nghÄ©a"]
    for k in keywords: 
        if k in question.lower(): return "vocab"
    return "grammar"

# --- 5. LOGIC CHÃNH ÄÃƒ NÃ‚NG Cáº¤P ---
def get_answer(question: str, type: str = None, session_id: str = "default"):
    history = get_chat_history(session_id)
    
    # --- CHECK 1: CÃ“ PHáº¢I XÃƒ GIAO KHÃ”NG? ---
    if is_chitchat(question):
        print("ğŸ’¬ Mode: Chitchat (KhÃ´ng tá»‘n cÃ´ng tra sÃ¡ch)")
        response = handle_chitchat(question, history)
        save_chat_history(session_id, question, response)
        return response
    # ----------------------------------------

    # Náº¿u khÃ´ng pháº£i xÃ£ giao -> Quy trÃ¬nh RAG bÃ¬nh thÆ°á»ng
    refined_question = contextualize_query(question, history)
    
    if not type or type == "auto":
        type = detect_intent(refined_question)
    
    collection_name = "grammar_collection" if type == "grammar" else "vocab_collection"
    print(f"ğŸ” TÃ¬m kiáº¿m '{refined_question}' trong {collection_name}")

    vector_store = Chroma(
        persist_directory=settings.CHROMA_DB_DIR,
        embedding_function=embedding_model,
        collection_name=collection_name
    )
    # TÄƒng k=10 Ä‘á»ƒ tÃ¬m sÃ¢u hÆ¡n
    retriever = vector_store.as_retriever(search_kwargs={"k": 10})
    
    # Láº¥y tÃ i liá»‡u
    docs = retriever.invoke(refined_question)
    context_text = "\n\n".join([doc.page_content for doc in docs])
    
    # Prompt RAG
    final_prompt = f"""
    Báº¡n lÃ  Lingora - má»™t giÃ¡o viÃªn Tiáº¿ng Anh chuyÃªn nghiá»‡p, thÃ¢n thiá»‡n vÃ  am hiá»ƒu sÃ¢u sáº¯c.
    Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  giáº£i thÃ­ch cÃ¢u há»i cho há»c viÃªn dá»±a trÃªn kiáº¿n thá»©c Ä‘Æ°á»£c cung cáº¥p.

    ğŸ”´ QUY Táº®C GIAO TIáº¾P (Báº®T BUá»˜C):
    1. **PHONG CÃCH Tá»° NHIÃŠN:** Tráº£ lá»i nhÆ° kiáº¿n thá»©c cá»§a chÃ­nh báº¡n. TUYá»†T Äá»I KHÃ”NG nÃ³i cÃ¡c cÃ¢u nhÆ°: "Dá»±a vÃ o sÃ¡ch", "Theo tÃ i liá»‡u", "Trang 295", "SÃ¡ch khÃ´ng Ä‘á» cáº­p".
    2. **Xá»¬ LÃ THIáº¾U THÃ”NG TIN:** Náº¿u ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p khÃ´ng Ä‘á»§, hÃ£y Tá»° Äá»˜NG bá»• sung báº±ng kiáº¿n thá»©c chuyÃªn mÃ´n cá»§a báº¡n má»™t cÃ¡ch trÃ´i cháº£y. Äá»«ng bÃ¡o cÃ¡o "SÃ¡ch thiáº¿u thÃ´ng tin".
    3. **KHÃ”NG TRÃCH DáºªN Sá» TRANG:** HÃ£y loáº¡i bá» má»i sá»‘ trang, tÃªn chÆ°Æ¡ng ra khá»i cÃ¢u tráº£ lá»i.
    4. **Cáº¤U TRÃšC:** TrÃ¬nh bÃ y ngáº¯n gá»n, dá»… hiá»ƒu, dÃ¹ng Bullet point náº¿u liá»‡t kÃª.

    [Kiáº¿n thá»©c ná»n (Ä‘á»ƒ báº¡n tham kháº£o)]:
    {context_text}

    [CÃ¢u há»i cá»§a há»c viÃªn]:
    {refined_question}

    ğŸ‘‰ HÃ£y tráº£ lá»i há»c viÃªn ngay (Tiáº¿ng Viá»‡t):
    """
    
    try:
        response_text = llm.invoke(final_prompt)
        save_chat_history(session_id, question, response_text)
        return response_text
    except Exception as e:
        return f"Lá»—i há»‡ thá»‘ng: {str(e)}"